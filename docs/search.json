[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/outliers/index.html",
    "href": "posts/outliers/index.html",
    "title": "Enhancing Machine Learning with Outlier Detection",
    "section": "",
    "text": "Code\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.metrics import precision_score\nimport numpy as np\n\n\n\n   Detecting and predicting outliers in machine learning is similar to finding the needles in a haystack; it’s about identifying data points that deviate from the norm. This process holds value as it helps ensure model accuracy and enhance overall performance. By spotting outliers, machine learning algorithms can avoid being disproportionately influenced by extreme data, thereby preventing skewed predictions or biased results. Removing outliers or treating them appropriately also aids in refining the model’s understanding of the underlying patterns within the data, leading to reliable predictions. In the end, spotting outliers boosts the trust and power of machine learning, helping it tackle real-life challenges better and offering valuable insights.\n\n\n  I found a dataset (here) containing measurements of 200 banknotes, each characterized by attributes like length, edge widths, margins, and diagonal length. Split evenly between genuine and counterfeit, this dataset serves as a resource for classifying banknotes based on their dimensional features.\n\n\n\nCode\ndf = pd.read_csv(\"../../python-notebooks/datasets/banknotes.csv\")\ndf.head()\n\n\n\n\n\n\n\n\n\nconterfeit\nLength\nLeft\nRight\nBottom\nTop\nDiagonal\n\n\n\n\n0\n0\n214.8\n131.0\n131.1\n9.0\n9.7\n141.0\n\n\n1\n0\n214.6\n129.7\n129.7\n8.1\n9.5\n141.7\n\n\n2\n0\n214.8\n129.7\n129.7\n8.7\n9.6\n142.2\n\n\n3\n0\n214.8\n129.7\n129.6\n7.5\n10.4\n142.0\n\n\n4\n0\n215.0\n129.6\n129.7\n10.4\n7.7\n141.8\n\n\n\n\n\n\n\n\n  We can use Boxplots to visualize the different columns in the dataset and view the outliers.\n\n\n\nCode\nsns.boxplot(data=df,x=df[\"Length\"])\nplt.title(\"Boxplot of Swiss Banknote Length \")\n\n\nText(0.5, 1.0, 'Boxplot of Swiss Banknote Length ')\n\n\n\n\n\n\n\nCode\nsns.boxplot(data=df,x=df[\"Left\"])\nplt.title(\"Boxplot of Swiss Banknote Left \")\n\n\nText(0.5, 1.0, 'Boxplot of Swiss Banknote Left ')\n\n\n\n\n\n\n\nCode\nsns.boxplot(data=df,x=df[\"Right\"])\nplt.title(\"Boxplot of Swiss Banknote Right \")\n\n\nText(0.5, 1.0, 'Boxplot of Swiss Banknote Right ')\n\n\n\n\n\n\n\nCode\nsns.boxplot(data=df,x=df[\"Bottom\"])\nplt.title(\"Boxplot of Swiss Banknote Bottom \")\n\n\nText(0.5, 1.0, 'Boxplot of Swiss Banknote Bottom ')\n\n\n\n\n\n\n\nCode\nsns.boxplot(data=df,x=df[\"Top\"])\nplt.title(\"Boxplot of Swiss Banknote Top \")\n\n\nText(0.5, 1.0, 'Boxplot of Swiss Banknote Top ')\n\n\n\n\n\n\n\nCode\nsns.boxplot(data=df,x=df[\"Diagonal\"])\nplt.title(\"Boxplot of Swiss Banknote Diagonal \")\n\n\nText(0.5, 1.0, 'Boxplot of Swiss Banknote Diagonal ')\n\n\n\n\n\n\n  As you can see the box plots for columns: Length, Left, Right, and Top all contain outliers. The next step involves training models using IsolationForest to address and handle these outliers effectively. Isolation is an anomaly detection algorithm used to identify outliers or anomalies in a dataset. It quickly spots anomalies by creating random trees that isolate outliers, as these anomalies typically need fewer steps to separate from the rest of the data. It’s a rapid and efficient method, particularly in datasets with many columns, like ours.\n\n\nX = df[['Length', 'Left', 'Right', 'Bottom', 'Top', 'Diagonal']]\ny = df['conterfeit']\nX_train, X_test, y_train, y_test = train_test_split(X, y, \ntest_size=0.33, random_state=42)\nclf = IsolationForest(random_state=0)\nclf.fit(X_train)\ny_pred = clf.predict(X_test)\n\n\n  After preparing for anomaly detection we use the train_test_split function, the dataset is divided into training and testing sets. An Isolation Forest model, a method commonly used for anomaly detection, is instantiated and trained on the training data. Finally, the trained model predicts anomalies in the test set, flagging instances that deviate from the norm. It also potentially identifies counterfeit banknotes based on their dimensional attributes.\n\n\n  Next, we will convert the predictions into binary values: 1 for outliers (identified by -1) and 0 for normal instances. It then calculates and prints the precision score, measuring how accurately the model identified outliers compared to the actual test labels.\n\n\n\nCode\npred = pd.DataFrame({'pred': y_pred})\npred['y_pred'] = np.where(pred['pred'] == -1, 1, 0)\ny_pred = pred['y_pred'] \nprint(\"Precision Score:\", precision_score(y_test, y_pred))\n\n\nPrecision Score: 0.625\n\n\n\n  The precision score of 0.625 underscores the model’s proficiency in accurately identifying outliers compared to the total predicted outliers. While demonstrating a substantial ability to spot anomalies, this score also hints at potential areas for refinement or enhancement in the anomaly detection process, ensuring more precise and reliable identification in future analyses.\n\n\n  In this case, outlier detection is important as it aids in distinguishing genuine banknotes from counterfeits based on their dimensional attributes. Identifying outliers among these measurements, such as unusual lengths or irregular margin widths, becomes essential in spotting counterfeit notes that deviate significantly from the norm. By leveraging outlier detection techniques like IsolationForest, the goal is to accurately pinpoint these anomalies, enhancing the ability to spot counterfeit banknotes.\n\n\n  In machine learning, outlier detection forms a fundamental preprocessing step. By cleaning the dataset of outliers, the following machine-learning models become more accurate in classifying genuine versus counterfeit banknotes. Removing outliers ensures that the model isn’t influenced by irregular data points, leading to more reliable predictions and a stronger performance overall. Therefore, outlier detection plays a pivotal role in refining the dataset and improving the effectiveness of machine learning algorithms.\n\n \n\nThank you!"
  },
  {
    "objectID": "posts/probability/index.html",
    "href": "posts/probability/index.html",
    "title": "Probablity with Diamonds",
    "section": "",
    "text": "Probability theory and random variables underpin machine learning by providing the mathematical framework for data analysis. They empower data scientists to evaluate the likelihood of outcomes in experiments. This includes examining data distributions, which classify numerical data into two distinct forms: discrete and continuous. Discrete data involves countable data points, as seen in experiments like rolling a dice and recording the resulting numbers. Utilizing Python’s diceRoll function enables the creation of a discrete dataset, visualizable through a histogram.\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nimport seaborn as sns\ndef diceRoll(n):\n    rollsList = []\n    for i in range(n):\n        two_dice = ( np.random.randint(1, 7) + np.random.randint(1, 7) )\n        rollsList.append(two_dice)\n    return rollsList\n\ndata = diceRoll(10)\n# print(data)\nbins = np.arange(14) - 0.5\nweights = np.ones_like(data) / len(data)\nplt.hist(data, weights=weights, bins=bins, color=\"blue\", rwidth=0.85)\nplt.title(\"Probability of Rolling 2 Dice Histogram\")\nplt.xlabel(\"Sum of 10 2 dice rolls\")\nplt.ylabel(\"Probability\")\nplt.xticks(range(1,14))\nplt.yticks(np.arange(0, .5, step=0.1))\n# Set axes limit\nplt.xlim(1,13)\nplt.show()\n\n\n\n\n\n\nCode\ndata = diceRoll(100)\nbins = np.arange(14) - 0.5\nweights = np.ones_like(data) / len(data)\nplt.hist(data, weights=weights, bins=bins, color=\"blue\", rwidth=0.85)\nplt.title(\"Probability of Rolling 2 Dice Histogram\")\nplt.xlabel(\"Sum of 100 2 dice rolls\")\nplt.ylabel(\"Probability\")\nplt.xticks(range(1,14))\nplt.yticks(np.arange(0, .5, step=0.1))\nplt.xlim(1,13)\nplt.show()\n\ndata = diceRoll(10000)\nbins = np.arange(14) - 0.5\nweights = np.ones_like(data) / len(data)\nplt.hist(data, weights=weights, bins=bins, color=\"blue\", rwidth=0.85)\nplt.title(\"Probability of Rolling 2 Dice Histogram\")\nplt.xlabel(\"Sum of 10000 2 dice rolls\")\nplt.ylabel(\"Probability\")\nplt.xticks(range(1,14))\nplt.yticks(np.arange(0, .5, step=0.1))\nplt.xlim(1,13)\nplt.show()\n\n\n\n\n\n\n\n\n\n  In the initial graph with a lower sample size, the plot appears more erratic. However, as the sample size increases, the distribution tends toward normality, resembling a bell curve typical of a Gaussian distribution. The function employs a random number generator, crucial for any dataset as randomness characterizes real-world scenarios. Random variables aid in quantifying these real-world experiments, particularly in capturing the central tendency of their distributions with larger sample sizes. With smaller samples, the impact of random variables is limited, as depicted in the initial graph. But as sample sizes grow, the graph takes shape, revealing the true median, which, in this case, is 7. These random variables, in this context, are considered discrete due to their limited range of possible values derived from a 6-sided dice, specifically ranging from 2 to 12.\n\n\n\nCode\ndiamonds = pd.read_csv('../../python-notebooks/datasets/diamonds.csv')\ndiamonds.sort_values(by=[\"price\"], ascending=True, inplace=True)\n\n\n\n  Continuous probability theory operates within a continuous data space, where values exist across a spectrum rather than discrete points. Take, for instance, prices—they are not confined to specific values but can vary extensively based on seller discretion. In a diamond dataset obtained online, the prices showcase significant variability. By examining the head (the lowest values) and the tail (the highest values) of the dataset, one can observe starkly different price points. Additionally, the range of prices can be considerable, allowing for the possibility of someone charging double the highest price for a diamond, illustrating the wide span of potential values within continuous datasets like these.\n\n\ndiamonds.head()\n\n\n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n0\n0.23\nIdeal\nE\nSI2\n61.5\n55.0\n326\n3.95\n3.98\n2.43\n\n\n1\n0.21\nPremium\nE\nSI1\n59.8\n61.0\n326\n3.89\n3.84\n2.31\n\n\n2\n0.23\nGood\nE\nVS1\n56.9\n65.0\n327\n4.05\n4.07\n2.31\n\n\n3\n0.29\nPremium\nI\nVS2\n62.4\n58.0\n334\n4.20\n4.23\n2.63\n\n\n4\n0.31\nGood\nJ\nSI2\n63.3\n58.0\n335\n4.34\n4.35\n2.75\n\n\n\n\n\n\n\n\ndiamonds.tail()\n\n\n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n27745\n2.00\nVery Good\nH\nSI1\n62.8\n57.0\n18803\n7.95\n8.00\n5.01\n\n\n27746\n2.07\nIdeal\nG\nSI2\n62.5\n55.0\n18804\n8.20\n8.13\n5.11\n\n\n27747\n1.51\nIdeal\nG\nIF\n61.7\n55.0\n18806\n7.37\n7.41\n4.56\n\n\n27748\n2.00\nVery Good\nG\nSI1\n63.5\n56.0\n18818\n7.90\n7.97\n5.04\n\n\n27749\n2.29\nPremium\nI\nVS2\n60.8\n60.0\n18823\n8.50\n8.47\n5.16\n\n\n\n\n\n\n\n\n  When we plot the data with price on the x-axis and count on the y-axis, it often shows an exponential distribution. This suggests that prices have no upper limit—they can keep rising as high as the seller wants.\n\n\nsns.displot(diamonds[\"price\"])\n\n\n\n\n\n  Probability and random variables are vital in machine learning. In supervised learning, we analyze the probability of X occurring based on observed variables Y. Understanding these distributions of observed data assists us in predicting outcomes. Hopefully, this blog has shed light on probability distributions, random variables, and their impact on machine learning.\n\n \n\nThank you!"
  },
  {
    "objectID": "posts/Classification/index.html",
    "href": "posts/Classification/index.html",
    "title": "KNN Empowers Fruit Classification Mastery",
    "section": "",
    "text": "Code\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.neighbors import KNeighborsClassifier  # get accuracy by KNN classifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\n\n\n   Classification in machine learning is a method used to teach models how to discern and assign predefined categories or classes to input data based on its features or attributes. It’s a foundational concept that serves as the backbone for various tasks, enabling machines to categorize or label data. This ability is crucial in making informed decisions or predictions.\n\n\n  Through classification, machines learn patterns from labeled examples. For instance, in the case of this fruit dataset (found here), the model learns to associate certain combinations of mass, width, height, and color score with specific fruit types (e.g., apple, mandarin, lemon). By identifying these patterns, the model can then categorize the fruits based on their features into the appropriate classes it has learned. This ability to generalize from known patterns to predict unseen data is the essence of classification.\n\n\n  The dataset contains information about various fruits, comprising attributes such as ‘fruit_label’ (a numerical label assigned to each fruit type), ‘fruit_name’ (the name of the fruit, which includes different types like apple, mandarin, etc.), ‘fruit_subtype’ (subcategories of certain fruits), ‘mass’ (the weight of the fruit in grams), ‘width’ (the width of the fruit), ‘height’ (the height of the fruit), and ‘color_score’ (a score representing the color of the fruit).\n\n\n\nCode\nfruits_df = pd.read_table('../../python-notebooks/datasets/fruit_data_with_colors.txt')\nfruits_df.head()\n\n\n\n\n\n\n\n\n\nfruit_label\nfruit_name\nfruit_subtype\nmass\nwidth\nheight\ncolor_score\n\n\n\n\n0\n1\napple\ngranny_smith\n192\n8.4\n7.3\n0.55\n\n\n1\n1\napple\ngranny_smith\n180\n8.0\n6.8\n0.59\n\n\n2\n1\napple\ngranny_smith\n176\n7.4\n7.2\n0.60\n\n\n3\n2\nmandarin\nmandarin\n86\n6.2\n4.7\n0.80\n\n\n4\n2\nmandarin\nmandarin\n84\n6.0\n4.6\n0.79\n\n\n\n\n\n\n\n\n  The graph below effectively visualizes the relationship between the width and height of different fruits, distinguishing each fruit type by color and labeling them in the legend for easy identification. Adjustments can be made to accommodate different fruit types and their respective attributes for visualization.\n\n\n\nCode\nfruit_colors = {'apple': 'green', 'mandarin': 'orange', 'lemon': 'red'}  # Add more colors if needed\n\nfor fruit, color in fruit_colors.items():\n    fruit_data = fruits_df[fruits_df['fruit_name'] == fruit]\n    plt.scatter(fruit_data['width'], fruit_data['height'], c=color, label=fruit)\n\nplt.xlabel('Width')\nplt.ylabel('Height')\nplt.title('Fruit Dataset - Width vs Height (Colored by Fruit Name)')\nplt.legend(title='Fruit Name')\nplt.show()\n\n\n\n\n\n\n  As we can see in the scatter plot above the fruits tend to group themselves based on height and width. Now lets train the data using train_test_split.\n\n\nfeature_names = ['mass', 'width', 'height', 'color_score']\nx=fruits_df[feature_names]\ny=fruits_df['fruit_label']\n\nx_train, x_test, y_train, y_test = train_test_split(x,y, random_state=0)\n\nprint(x_train[:3])\n\nscaler = MinMaxScaler()\nx_train=scaler.fit_transform(x_train)\nx_test= scaler.transform(x_test)\n\nprint(\"\\nAfter scaling\\n\")\nprint(x_train[:3])\n\n    mass  width  height  color_score\n42   154    7.2     7.2         0.82\n48   174    7.3    10.1         0.72\n7     76    5.8     4.0         0.81\n\nAfter scaling\n\n[[0.27857143 0.41176471 0.49230769 0.72972973]\n [0.35       0.44117647 0.93846154 0.45945946]\n [0.         0.         0.         0.7027027 ]]\n\n\n\n  After scaling seen above, the values displayed represent the same three fruits, but the feature values have been transformed. Each feature value is now proportionally represented between 0 and 1 based on its original range in the dataset. These scaled values are essential for model training as they provide a consistent scale for all features, preventing one particular feature from dominating the learning process due to its higher magnitude. This standardized representation allows the model to learn effectively from the data and make unbiased predictions without the influence of varying feature scales.\n\n\n  Now we will find the accuracy of the training data on a KNeighborsClassifier. The accuracy scores on both the training and test sets help assess the KNN model’s performance. A high accuracy score on the training set suggests that the model learned well from the training data. However, the test set accuracy is more crucial, as it indicates how effectively the model can predict fruit types on new, unseen data, reflecting its overall performance and ability to generalize to new instances. Comparing these scores helps gauge the model’s performance and assess the potential for overfitting (if the training accuracy is significantly higher than the test accuracy).\n\n\n\nCode\nknn = KNeighborsClassifier()\nknn.fit(x_train, y_train)\n\nprint('Accuracy of KNN classifier on training set:{:.2f}'\n     .format(knn.score(x_train, y_train)))\n\nprint('Accuracy of KNN Classifier on test set:{:.2f}'\n     .format(knn.score(x_test, y_test)))\n\n\nAccuracy of KNN classifier on training set:0.95\nAccuracy of KNN Classifier on test set:1.00\n\n\n\n  The accuracy of the training set suggests that the KNN classifier achieved an accuracy of 95%. The accuracy score represents the proportion of correctly predicted fruit labels by the model compared to the total number of fruits in the training dataset. A score of 0.95 suggests that the model accurately predicted the fruit types for 95% of the instances in the dataset used for training.\n\n\n  The accuracy of the test set indicates that the KNN classifier achieved a perfect accuracy score of 100%. The accuracy score on the test set represents the model’s ability to generalize and make accurate predictions on new, unseen data (not used during training). A score of 100% means that the model correctly classified all the fruit instances in the test dataset.\n\n\n  In summary, the KNN classifier exhibited strong performance on both the training and test datasets. A high accuracy score on the test set (1.00) suggests that the model generalized exceptionally well to new data, correctly predicting all fruit types. Meanwhile, the slightly lower training set accuracy (0.95) indicates that the model performed slightly worse on the data it was trained on, potentially suggesting a small amount of overfitting or a minor limitation in capturing certain complexities present in the training data. Overall, these scores suggest that the KNN classifier is performing very well in classifying fruit types based on the provided features.\n\n\n  In conclusion, these results underline the importance of effective model training and evaluation. The strong performance of the model on the test set indicates its ability to generalize well to unseen data, a crucial aspect in real-world applications. Classification, as demonstrated here, plays a pivotal role in various domains, allowing machines to classify data into distinct categories or classes, enabling tasks like spam detection, medical diagnosis, image recognition, and more. High-performing classification models facilitate informed decision-making, automation, and pattern recognition, showcasing their critical role in advancing machine learning applications.\n\n \n\nThank you!\n\n\nReferences: IBM KNN | Classification"
  },
  {
    "objectID": "posts/cluster/index.html",
    "href": "posts/cluster/index.html",
    "title": "Unearthing Insights: Clustering Earthquake Data for Patterns",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.cluster import DBSCAN\nimport warnings\n\n\n\n  Clustering is pivotal in machine learning for its ability to unearth hidden patterns within datasets. By employing algorithms that group similar data points together based on shared characteristics, clustering enables us to discern valuable insights that might otherwise remain obscured. This process is particularly crucial in unsupervised learning, where the data lacks explicit labels or categories. Through clustering, we can reveal natural structures, identify trends, and segment data points, aiding in tasks like customer segmentation, anomaly detection, and image recognition. Essentially, clustering acts as a guiding light, enabling us to navigate and understand complex data landscapes, facilitating better decision-making and predictive modeling in machine learning applications.\n\n\n   For example, I found a dataset (here) encompassing global earthquake records from 2016. To focus on studying earthquakes ranging from moderate to severe intensity (Magnitude 3 or higher), clustering techniques serve as a valuable approach. In this scenario, leveraging clustering techniques becomes instrumental in extracting meaningful insights. One such clustering method suited for this task is DBSCAN (Density-Based Spatial Clustering of Applications with Noise). DBSCAN operates by identifying and grouping data points based on their density within a given proximity. Imagine plotting earthquake occurrences on a geographical map, with each point representing a seismic event. DBSCAN would efficiently cluster together points that are densely located, signifying regions or areas where earthquakes of comparable magnitudes cluster. Through this method, earthquakes nearby are visually represented with the same color, aiding in recognizing spatial patterns and clusters of seismic activity.\n\n\n  In the code below, we start by refining our dataset, extracting the essential details such as Magnitude, Longitude, and Latitude. We filter out earthquakes with a magnitude under 3. Then, we implement DBSCAN. By setting ‘eps’ to 6, we define the proximity for points to be considered neighbors. Additionally, ‘min_samples’ is set to 4, requiring a minimum of four earthquakes to form a cluster. This configuration leads to the formation of sparser clusters due to the specified criteria.\n\n\ndf_full = pd.read_csv(\"../../python-notebooks/datasets/earthquakes.csv\")\ndf = df_full[['Latitude','Longitude','Magnitude']]\ndf = df[df.Magnitude &gt;= 3]\n\ndbscan = DBSCAN(eps = 6, min_samples=4).fit(df) # fitting the model\nlabels = dbscan.labels_ # getting the labels\n\n\n  The graph below illustrates our plotted data, with distinct colors representing different clusters, while isolated red points denote random earthquakes not associated with any cluster. Larger points on the graph indicate earthquakes of higher magnitude. This visualization provides a clear overview, enabling us to identify regions exhibiting significant earthquake activity.\n\n\n\nCode\nworldmap = gpd.read_file(gpd.datasets.get_path(\"naturalearth_lowres\"))\n# Creating axes and plotting world map\nfig, ax = plt.subplots(figsize=(18, 10))\nworldmap.plot(color=\"gray\", ax=ax)\n\nsns.scatterplot(x=df.Longitude, y=df.Latitude, hue=labels, palette='Set1', size=df.Magnitude, sizes=(10,200)) # plotting the clusters\n\nax. legend_ = None\nplt.title(\"Earthquakes clusters with magnitudes greater than 3 in 2016\")\nplt.xlabel(\"Longitude\") # X-axis label\nplt.ylabel(\"Latitude\") # Y-axis label\n\nplt.show() # showing the plot\nwarnings.filterwarnings('ignore')\n\n\n\n\n\n\n  In this example, clustering served to group earthquakes based on their spatial proximity—a excellent application of unsupervised learning. Starting with raw data, we organized data points into clusters by identifying similarities. Subsequently, we explored these clusters to discern patterns within the data.\n\n\n  It’s important to note that the algorithm itself doesn’t interpret the data’s meaning. However, when visualized, particularly on a map, distinct patterns emerge within each cluster. This visualization vividly demonstrates that certain regions of the world are more susceptible to seismic activity than others.\n\n\n  I hope this blog post aids in your comprehension of Clustering and the realm of unsupervised learning, showcasing its utility in uncovering patterns and insights from unstructured data.\n\n \n\nThank you!\n\n\nReferences: DBSCAN | Google Clustering  |  Inspiration"
  },
  {
    "objectID": "posts/Linear Regression/index.html",
    "href": "posts/Linear Regression/index.html",
    "title": "Decoding Data with Linear Regression: Predictions & Insights",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\n\n\n  In the realm of machine learning, predictive modeling stands as a strong force for reshaping industries and driving innovation. At the core of this field lies linear regression, a fundamental yet powerful statistical technique that serves as a cornerstone for making predictions based on data. Imagine we have data points that represent things like house prices and their sizes. Linear regression helps find a straight line that best fits these points, allowing you to predict the price of a house based on its size. In machine learning predicting this outcome based on the input data can help us predict future prices or see trends. Linear regression is commonly used in all different types of industries, it helps companies make informed decisions.\n\n\n  Marketing and advertising industries heavily rely on trend analysis. In pursuit of understanding these trends, I obtained a dataset from Kaggle.com that details sales figures correlated with expenditures on TV, newspaper, and radio advertising.\n\n\n\nCode\nadvertisement_df = pd.read_csv(\"../../python-notebooks/datasets/advertising.csv\")\nadvertisement_df.head()\n\n\n\n\n\n\n\n\n\nTV\nRadio\nNewspaper\nSales\n\n\n\n\n0\n230.1\n37.8\n69.2\n22.1\n\n\n1\n44.5\n39.3\n45.1\n10.4\n\n\n2\n17.2\n45.9\n69.3\n12.0\n\n\n3\n151.5\n41.3\n58.5\n16.5\n\n\n4\n180.8\n10.8\n58.4\n17.9\n\n\n\n\n\n\n\n\n  Our primary focus lies in assessing the correlation between TV advertisement spending, radio advertisement and sales. Initially, we’ll improve the dataset by eliminating irrelevant columns and then juxtapose the pertinent data for analysis.\n\n\n\nCode\nadvertisement_df.drop(columns=['Newspaper'], inplace = True)\nsns.pairplot(advertisement_df, x_vars=['TV', 'Radio'], y_vars='Sales', height=4, aspect=1, kind='scatter')\nplt.show()\n\n\n\n\n\n\n  Upon observation, the Radio data exhibits considerably higher dispersion compared to the TV data. Moving forward, our aim is to train a linear model. Employing the ‘train_test_split’ technique, we’ll allocate 70% of the data for training and 30% for testing, separately for both TV and radio datasets.\n\n\nx_tv = advertisement_df[['TV']]\ny_tv = advertisement_df['Sales']\nx_train, x_test, y_train, y_test = train_test_split(x_tv, y_tv, test_size = 0.3, random_state = 100)\nx_radio = advertisement_df[['Radio']]\ny_radio = advertisement_df['Sales']\nx_train_radio, x_test_radio, y_train_radio, y_test_radio = train_test_split(x_radio, y_radio, test_size = 0.3, random_state = 100)\nlrm_tv = LinearRegression().fit(x_train, y_train)\nlrm_radio = LinearRegression().fit(x_train_radio, y_train_radio) \n\n\n\nCode\nplt.scatter(x_test,y_test)\nplt.xlabel(\"$ amount spent on TV advertising\")\nplt.ylabel(\"Number of sales\")\n\nplt.title(\"Number of sales per amount spent on TV ads\")\nplt.show()\n\n\n\n\n\n\n\nCode\nplt.scatter(x_test_radio,y_test_radio)\nplt.xlabel(\"$ amount spent on Radio advertising\")\nplt.ylabel(\"Number of sales\")\nplt.title(\"Number of sales per amount spent on Radio ads\")\nplt.show()\n\n\n\n\n\n\n  Our visual analysis echoes previous observations. The TV dataset exhibits a more compact ascending structure, suggesting a positive linear relationship, while the radio dataset diverges noticeably. Subsequently, we’ll utilize the trained linear regression model to predict sales for both TV and radio advertising channels.\n\n\ny_pred_lrm_tv= lrm_tv.predict(x_test)\ny_pred_lrm_radio= lrm_radio.predict(x_test_radio)\nlrm_diff = pd.DataFrame({'Actual Sales': y_test, 'Predicted Sales for TV': y_pred_lrm_tv, \n                         'Predicted Sales for Radio': y_pred_lrm_radio})\nlrm_diff.head()\n\n\n\n\n\n\n\n\nActual Sales\nPredicted Sales for TV\nPredicted Sales for Radio\n\n\n\n\n126\n6.6\n7.374140\n16.985157\n\n\n104\n20.7\n19.941482\n16.415126\n\n\n99\n17.2\n14.323269\n17.332133\n\n\n92\n19.4\n18.823294\n16.315990\n\n\n111\n21.8\n20.132392\n16.873629\n\n\n\n\n\n\n\n\n  While the model’s predictions for TV advertisement sales align closely with actual figures, its performance in predicting radio advertisement sales falls significantly short. This discrepancy signals that a linear model might not be suitable for radio advertising, which we’ll corroborate by graphing the data.\n\n\n\nCode\nplt.scatter(x_test,y_test)\nplt.plot(x_test, y_pred_lrm_tv, 'Red')\nplt.xlabel(\"$ amount spent on TV advertising\")\nplt.ylabel(\"Number of sales\")\nplt.title(\"Number of sales per amount spent on TV ads\")\n\n\nText(0.5, 1.0, 'Number of sales per amount spent on TV ads')\n\n\n\n\n\n\n\nCode\nplt.scatter(x_test_radio,y_test_radio)\nplt.plot(x_test_radio, y_pred_lrm_radio, 'Red')\nplt.xlabel(\"$ amount spent on Radio advertising\")\nplt.ylabel(\"Number of sales\")\nplt.title(\"Number of sales per amount spent on Radio ads\")\nplt.show()\n\n\n\n\n\n\n  Comparing the two graphs highlights the clear linear relationship between TV advertisement spending and sales, contrasting with the inconsistency in radio advertising. The limitations of the linear model’s fit to the radio data suggest a need for non-linear regression, indicating a potential requirement for a more curved or complex model to better represent this relationship.\n\n\n  Insights learned from this visualization suggest that the company’s sales potential significantly benefits from TV advertising over radio. Armed with this data, a marketer could assert, “Increased investment in TV advertising directly correlates with higher sales.” Conversely, the erratic nature of radio advertising data may prompt a recommendation to reconsider its budget allocation due to its scattered and less predictable impact on sales.\n\n\n  Linear regression, a fundamental tool in machine learning, enables us to decipher relationships between variables. Our analysis highlighted its efficacy in predicting sales based on TV advertising spending. In conclusion, linear regression helps us with uncovering correlations, making predictions, guiding decisions, and so much more, becoming a versatile tool across diverse fields of study and application.\n\n \n\nThank you!\n\n\nReferences: IBM Linear Regression"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ellie’s Machine Learning Blog",
    "section": "",
    "text": "Enhancing Machine Learning with Outlier Detection\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nNov 30, 2023\n\n\nEllie Woodward\n\n\n\n\n\n\n  \n\n\n\n\nKNN Empowers Fruit Classification Mastery\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2023\n\n\nEllie Woodward\n\n\n\n\n\n\n  \n\n\n\n\nDecoding Data with Linear Regression: Predictions & Insights\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nNov 20, 2023\n\n\nEllie Woodward\n\n\n\n\n\n\n  \n\n\n\n\nUnearthing Insights: Clustering Earthquake Data for Patterns\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nNov 10, 2023\n\n\nEllie Woodward\n\n\n\n\n\n\n  \n\n\n\n\nProbablity with Diamonds\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2023\n\n\nEllie Woodward\n\n\n\n\n\n\nNo matching items"
  }
]