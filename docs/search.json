[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/outliers/index.html",
    "href": "posts/outliers/index.html",
    "title": "Outlier Detection",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport numpy as np\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nimport seaborn as sns\nfrom sklearn.cluster import KMeans\nimport warnings\n\n\n\n   Full Blog post coming by end of day 12/7\n\n\n  \n\n\n  \n\n\n  \n\n \n\nThank you!"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/cluster/index.html",
    "href": "posts/cluster/index.html",
    "title": "Unearthing Insights: Clustering Earthquake Data for Patterns",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.cluster import DBSCAN\nimport warnings\n\n\n\n  Clustering is pivotal in machine learning for its ability to unearth hidden patterns within datasets. By employing algorithms that group similar data points together based on shared characteristics, clustering enables us to discern valuable insights that might otherwise remain obscured. This process is particularly crucial in unsupervised learning, where the data lacks explicit labels or categories. Through clustering, we can reveal natural structures, identify trends, and segment data points, aiding in tasks like customer segmentation, anomaly detection, and image recognition. Essentially, clustering acts as a guiding light, enabling us to navigate and understand complex data landscapes, facilitating better decision-making and predictive modeling in machine learning applications.\n\n\n   For example, I found a dataset (here) encompassing global earthquake records from 2016. To focus on studying earthquakes ranging from moderate to severe intensity (Magnitude 3 or higher), clustering techniques serve as a valuable approach. In this scenario, leveraging clustering techniques becomes instrumental in extracting meaningful insights. One such clustering method suited for this task is DBSCAN (Density-Based Spatial Clustering of Applications with Noise). DBSCAN operates by identifying and grouping data points based on their density within a given proximity. Imagine plotting earthquake occurrences on a geographical map, with each point representing a seismic event. DBSCAN would efficiently cluster together points that are densely located, signifying regions or areas where earthquakes of comparable magnitudes cluster. Through this method, earthquakes nearby are visually represented with the same color, aiding in recognizing spatial patterns and clusters of seismic activity.\n\n\n  In the code below, we start by refining our dataset, extracting the essential details such as Magnitude, Longitude, and Latitude. We filter out earthquakes with a magnitude under 3. Then, we implement DBSCAN. By setting ‘eps’ to 6, we define the proximity for points to be considered neighbors. Additionally, ‘min_samples’ is set to 4, requiring a minimum of four earthquakes to form a cluster. This configuration leads to the formation of sparser clusters due to the specified criteria.\n\n\ndf_full = pd.read_csv(\"../../python-notebooks/datasets/earthquakes.csv\")\ndf = df_full[['Latitude','Longitude','Magnitude']]\ndf = df[df.Magnitude &gt;= 3]\n\ndbscan = DBSCAN(eps = 6, min_samples=4).fit(df) # fitting the model\nlabels = dbscan.labels_ # getting the labels\n\n\n  The graph below illustrates our plotted data, with distinct colors representing different clusters, while isolated red points denote random earthquakes not associated with any cluster. Larger points on the graph indicate earthquakes of higher magnitude. This visualization provides a clear overview, enabling us to identify regions exhibiting significant earthquake activity.\n\n\n\nCode\nworldmap = gpd.read_file(gpd.datasets.get_path(\"naturalearth_lowres\"))\n# Creating axes and plotting world map\nfig, ax = plt.subplots(figsize=(18, 10))\nworldmap.plot(color=\"gray\", ax=ax)\n\nsns.scatterplot(x=df.Longitude, y=df.Latitude, hue=labels, palette='Set1', size=df.Magnitude, sizes=(10,200)) # plotting the clusters\n\nax. legend_ = None\nplt.title(\"Earthquakes clusters with magnitudes greater than 3 in 2016\")\nplt.xlabel(\"Longitude\") # X-axis label\nplt.ylabel(\"Latitude\") # Y-axis label\n\nplt.show() # showing the plot\nwarnings.filterwarnings('ignore')\n\n\n\n\n\n\n  In this example, clustering served to group earthquakes based on their spatial proximity—a excellent application of unsupervised learning. Starting with raw data, we organized data points into clusters by identifying similarities. Subsequently, we explored these clusters to discern patterns within the data.\n\n\n  It’s important to note that the algorithm itself doesn’t interpret the data’s meaning. However, when visualized, particularly on a map, distinct patterns emerge within each cluster. This visualization vividly demonstrates that certain regions of the world are more susceptible to seismic activity than others.\n\n\n  I hope this blog post aids in your comprehension of Clustering and the realm of unsupervised learning, showcasing its utility in uncovering patterns and insights from unstructured data.\n\n \n\nThank you!\n\n\nReferences: DBSCAN | Google Clustering  |  Inspiration"
  },
  {
    "objectID": "posts/Classification/index.html",
    "href": "posts/Classification/index.html",
    "title": "Classification",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport numpy as np\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nimport seaborn as sns\nfrom sklearn.cluster import KMeans\nimport warnings\n\n\n\n   Full Blog post coming by end of day 12/7\n\n\n  \n\n\n  \n\n\n  \n\n \n\nThank you!"
  },
  {
    "objectID": "posts/probability/index.html",
    "href": "posts/probability/index.html",
    "title": "Probablity",
    "section": "",
    "text": "Probability Theory and Random variables are important to machine learning because it is the mathematical foundation for analyzing data. This subject allows data scientists to assess the certainty of outcomes in experiments. One of the ways they can do this is by examining data distributions. Numerical data can come in 2 different forms, discrete or continuous. Discrete data deals with data points that come in a countable form. A good example of a discrete experiment is rolling a dice and seeing what number you get on each roll. Using the Python function diceRoll we can create a Discrete dataset which we can then visualize using a histogram.\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nimport seaborn as sns\ndef diceRoll(n):\n    rollsList = []\n    for i in range(n):\n        two_dice = ( np.random.randint(1, 7) + np.random.randint(1, 7) )\n        rollsList.append(two_dice)\n    return rollsList\n\ndata = diceRoll(10)\n# print(data)\nbins = np.arange(14) - 0.5\nweights = np.ones_like(data) / len(data)\nplt.hist(data, weights=weights, bins=bins, color=\"blue\", rwidth=0.85)\nplt.title(\"Probability of Rolling 2 Dice Histogram\")\nplt.xlabel(\"Sum of 10 2 dice rolls\")\nplt.ylabel(\"Probability\")\nplt.xticks(range(1,14))\nplt.yticks(np.arange(0, .5, step=0.1))\n# Set axes limit\nplt.xlim(1,13)\nplt.show()\n\n\n\n\n\n\nCode\ndata = diceRoll(100)\nbins = np.arange(14) - 0.5\nweights = np.ones_like(data) / len(data)\nplt.hist(data, weights=weights, bins=bins, color=\"blue\", rwidth=0.85)\nplt.title(\"Probability of Rolling 2 Dice Histogram\")\nplt.xlabel(\"Sum of 100 2 dice rolls\")\nplt.ylabel(\"Probability\")\nplt.xticks(range(1,14))\nplt.yticks(np.arange(0, .5, step=0.1))\nplt.xlim(1,13)\nplt.show()\n\ndata = diceRoll(10000)\nbins = np.arange(14) - 0.5\nweights = np.ones_like(data) / len(data)\nplt.hist(data, weights=weights, bins=bins, color=\"blue\", rwidth=0.85)\nplt.title(\"Probability of Rolling 2 Dice Histogram\")\nplt.xlabel(\"Sum of 10000 2 dice rolls\")\nplt.ylabel(\"Probability\")\nplt.xticks(range(1,14))\nplt.yticks(np.arange(0, .5, step=0.1))\nplt.xlim(1,13)\nplt.show()\n\n\n\n\n\n\n\n\n\n  As you can see here with the lower sample size the graph looks more erratic but as the same size increases the distribution becomes more normal. A normal or Gaussian distribution takes on the shape of a bell curve. Also as you can see in the function we are using a random number generator. This randomness is important to any dataset. Random variables help us quantify real-world experiments. They help us capture the center of that random variable distribution with large sample sizes. When we have small sample sizes the random variables don’t help us much, as we can see in the first graph. Once they increase the graph starts to form and we see the true median, which is 7. In this case, our random variables are considered discrete because they only take in values the possible values from a 6-sided dice, 2 - 12.\n\n\n\nCode\ndiamonds = pd.read_csv('../../python-notebooks/datasets/diamonds.csv')\ndiamonds.sort_values(by=[\"price\"], ascending=True, inplace=True)\n\n\n\n  Continuous probability theory deals with data in a continuous data space. A good example of a continuous data set is price. Price is not a fixed number of values, it can be anything that the seller wants to set it as. In a diamond dataset, I have found online, the prices vary drastically. As you can see from the head and the tail of the data set the prices are 2 completely different numbers, but someone could try and charge double the highest price for a diamond.\n\n\ndiamonds.head()\n\n\n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n0\n0.23\nIdeal\nE\nSI2\n61.5\n55.0\n326\n3.95\n3.98\n2.43\n\n\n1\n0.21\nPremium\nE\nSI1\n59.8\n61.0\n326\n3.89\n3.84\n2.31\n\n\n2\n0.23\nGood\nE\nVS1\n56.9\n65.0\n327\n4.05\n4.07\n2.31\n\n\n3\n0.29\nPremium\nI\nVS2\n62.4\n58.0\n334\n4.20\n4.23\n2.63\n\n\n4\n0.31\nGood\nJ\nSI2\n63.3\n58.0\n335\n4.34\n4.35\n2.75\n\n\n\n\n\n\n\n\ndiamonds.tail()\n\n\n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n27745\n2.00\nVery Good\nH\nSI1\n62.8\n57.0\n18803\n7.95\n8.00\n5.01\n\n\n27746\n2.07\nIdeal\nG\nSI2\n62.5\n55.0\n18804\n8.20\n8.13\n5.11\n\n\n27747\n1.51\nIdeal\nG\nIF\n61.7\n55.0\n18806\n7.37\n7.41\n4.56\n\n\n27748\n2.00\nVery Good\nG\nSI1\n63.5\n56.0\n18818\n7.90\n7.97\n5.04\n\n\n27749\n2.29\nPremium\nI\nVS2\n60.8\n60.0\n18823\n8.50\n8.47\n5.16\n\n\n\n\n\n\n\n\n  When we put the data into a distribution plot with price on the x-axis and count on the y-axis we can see a graph that makes a nice exponential distribution. This distribution signals to us that the price can go on forever and be as high as the seller wants.\n\n\nsns.displot(diamonds[\"price\"])\n\n\n\n\n\n  Probability and Random variables are important machine learning concepts. In supervised learning, for example, we like to look at the probability of X happening given observed variables Y. These distributions of observed data help us decipher what the outcome should be. Hopefully this blog has helped you understand more about probability distributions, random variables and their influence on machine learning.\n\n \n\nThank you!"
  },
  {
    "objectID": "posts/Linear Regression/index.html",
    "href": "posts/Linear Regression/index.html",
    "title": "Decoding Data with Linear Regression: Predictions & Insights",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\n\n\n  In the realm of machine learning, predictive modeling stands as a strong force for reshaping industries and driving innovation. At the core of this field lies linear regression, a fundamental yet powerful statistical technique that serves as a cornerstone for making predictions based on data. Imagine we have data points that represent things like house prices and their sizes. Linear regression helps find a straight line that best fits these points, allowing you to predict the price of a house based on its size. In machine learning predicting this outcome based on the input data can help us predict future prices or see trends. Linear regression is commonly used in all different types of industries, it helps companies make informed decisions.\n\n\n  Marketing and advertising industries heavily rely on trend analysis. In pursuit of understanding these trends, I obtained a dataset from Kaggle.com that details sales figures correlated with expenditures on TV, newspaper, and radio advertising.\n\n\n\nCode\nadvertisement_df = pd.read_csv(\"../../python-notebooks/datasets/advertising.csv\")\nadvertisement_df.head()\n\n\n\n\n\n\n\n\n\nTV\nRadio\nNewspaper\nSales\n\n\n\n\n0\n230.1\n37.8\n69.2\n22.1\n\n\n1\n44.5\n39.3\n45.1\n10.4\n\n\n2\n17.2\n45.9\n69.3\n12.0\n\n\n3\n151.5\n41.3\n58.5\n16.5\n\n\n4\n180.8\n10.8\n58.4\n17.9\n\n\n\n\n\n\n\n\n  Our primary focus lies in assessing the correlation between TV advertisement spending, radio advertisement and sales. Initially, we’ll improve the dataset by eliminating irrelevant columns and then juxtapose the pertinent data for analysis.\n\n\n\nCode\nadvertisement_df.drop(columns=['Newspaper'], inplace = True)\nsns.pairplot(advertisement_df, x_vars=['TV', 'Radio'], y_vars='Sales', height=4, aspect=1, kind='scatter')\nplt.show()\n\n\n\n\n\n\n  Upon observation, the Radio data exhibits considerably higher dispersion compared to the TV data. Moving forward, our aim is to train a linear model. Employing the ‘train_test_split’ technique, we’ll allocate 70% of the data for training and 30% for testing, separately for both TV and radio datasets.\n\n\nx_tv = advertisement_df[['TV']]\ny_tv = advertisement_df['Sales']\nx_train, x_test, y_train, y_test = train_test_split(x_tv, y_tv, test_size = 0.3, random_state = 100)\nx_radio = advertisement_df[['Radio']]\ny_radio = advertisement_df['Sales']\nx_train_radio, x_test_radio, y_train_radio, y_test_radio = train_test_split(x_radio, y_radio, test_size = 0.3, random_state = 100)\nlrm_tv = LinearRegression().fit(x_train, y_train)\nlrm_radio = LinearRegression().fit(x_train_radio, y_train_radio) \n\n\n\nCode\nplt.scatter(x_test,y_test)\nplt.xlabel(\"$ amount spent on TV advertising\")\nplt.ylabel(\"Number of sales\")\n\nplt.title(\"Number of sales per amount spent on TV ads\")\nplt.show()\n\n\n\n\n\n\n\nCode\nplt.scatter(x_test_radio,y_test_radio)\nplt.xlabel(\"$ amount spent on Radio advertising\")\nplt.ylabel(\"Number of sales\")\nplt.title(\"Number of sales per amount spent on Radio ads\")\nplt.show()\n\n\n\n\n\n\n  Our visual analysis echoes previous observations. The TV dataset exhibits a more compact ascending structure, suggesting a positive linear relationship, while the radio dataset diverges noticeably. Subsequently, we’ll utilize the trained linear regression model to predict sales for both TV and radio advertising channels.\n\n\ny_pred_lrm_tv= lrm_tv.predict(x_test)\ny_pred_lrm_radio= lrm_radio.predict(x_test_radio)\nlrm_diff = pd.DataFrame({'Actual Sales': y_test, 'Predicted Sales for TV': y_pred_lrm_tv, \n                         'Predicted Sales for Radio': y_pred_lrm_radio})\nlrm_diff.head()\n\n\n\n\n\n\n\n\nActual Sales\nPredicted Sales for TV\nPredicted Sales for Radio\n\n\n\n\n126\n6.6\n7.374140\n16.985157\n\n\n104\n20.7\n19.941482\n16.415126\n\n\n99\n17.2\n14.323269\n17.332133\n\n\n92\n19.4\n18.823294\n16.315990\n\n\n111\n21.8\n20.132392\n16.873629\n\n\n\n\n\n\n\n\n  While the model’s predictions for TV advertisement sales align closely with actual figures, its performance in predicting radio advertisement sales falls significantly short. This discrepancy signals that a linear model might not be suitable for radio advertising, which we’ll corroborate by graphing the data.\n\n\n\nCode\nplt.scatter(x_test,y_test)\nplt.plot(x_test, y_pred_lrm_tv, 'Red')\nplt.xlabel(\"$ amount spent on TV advertising\")\nplt.ylabel(\"Number of sales\")\nplt.title(\"Number of sales per amount spent on TV ads\")\n\n\nText(0.5, 1.0, 'Number of sales per amount spent on TV ads')\n\n\n\n\n\n\n\nCode\nplt.scatter(x_test_radio,y_test_radio)\nplt.plot(x_test_radio, y_pred_lrm_radio, 'Red')\nplt.xlabel(\"$ amount spent on Radio advertising\")\nplt.ylabel(\"Number of sales\")\nplt.title(\"Number of sales per amount spent on Radio ads\")\nplt.show()\n\n\n\n\n\n\n  Comparing the two graphs highlights the clear linear relationship between TV advertisement spending and sales, contrasting with the inconsistency in radio advertising. The limitations of the linear model’s fit to the radio data suggest a need for non-linear regression, indicating a potential requirement for a more curved or complex model to better represent this relationship.\n\n\n  Insights learned from this visualization suggest that the company’s sales potential significantly benefits from TV advertising over radio. Armed with this data, a marketer could assert, “Increased investment in TV advertising directly correlates with higher sales.” Conversely, the erratic nature of radio advertising data may prompt a recommendation to reconsider its budget allocation due to its scattered and less predictable impact on sales.\n\n\n  Linear regression, a fundamental tool in machine learning, enables us to decipher relationships between variables. Our analysis highlighted its efficacy in predicting sales based on TV advertising spending. In conclusion, linear regression helps us with uncovering correlations, making predictions, guiding decisions, and so much more, becoming a versatile tool across diverse fields of study and application.\n\n \n\nThank you!\n\n\nReferences: IBM Linear Regression"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ellie’s Machine Learning Blog",
    "section": "",
    "text": "Outlier Detection\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nNov 30, 2023\n\n\nEllie Woodward\n\n\n\n\n\n\n  \n\n\n\n\nClassification\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2023\n\n\nEllie Woodward\n\n\n\n\n\n\n  \n\n\n\n\nDecoding Data with Linear Regression: Predictions & Insights\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nNov 20, 2023\n\n\nEllie Woodward\n\n\n\n\n\n\n  \n\n\n\n\nUnearthing Insights: Clustering Earthquake Data for Patterns\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nNov 10, 2023\n\n\nEllie Woodward\n\n\n\n\n\n\n  \n\n\n\n\nProbablity\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2023\n\n\nEllie Woodward\n\n\n\n\n\n\nNo matching items"
  }
]