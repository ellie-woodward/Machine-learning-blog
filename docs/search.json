[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/outliers/index.html",
    "href": "posts/outliers/index.html",
    "title": "Outlier Detection",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport numpy as np\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nimport seaborn as sns\nfrom sklearn.cluster import KMeans\nimport warnings\n\n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n \n\nThank you!"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/cluster/index.html",
    "href": "posts/cluster/index.html",
    "title": "Clustering Earthquake Data",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.cluster import DBSCAN\nimport warnings\n\n\n\n  Clustering is a concept that is used by people every day. We use clustering when we are organizing objects into meaningful categories. Think about the grocery store. All the produce is organized into one aisle, chips in another, and alcohol in a different part. Then we label those sections so when we want to find an apple, for example, we know to look in the produce section. This concept is also important for machine learning. When doing unsupervised learning we group unlabeled examples so we can visualize patterns in a dataset.\n\n\n  For example, if we had a dataset that held all the registered earthquakes around the world in 2016 and we wanted to study the mild to extreme earthquakes (Magnitude 3 or above), we could use clustering. We can use DBSCAN (Density-Based Spatial Clustering of Applications with Noise) to group together earthquakes based on their density. DBSCAN will group together points on the plot that are close to each other, it does this by making the points the same color.\n\n\n  In the code below we are first managing our data. We are getting only the necessary points of interest by receiving the Magintude, Longitude and Latitude data then we are taking out all earthquakes that are registered under 3 Magintude. After that we are using DBSCAN. We are setting eps to 6, which means we are declaring how far away a point can be to still be considered a neighbor. Then we set the min samples to 4. This means there needs to be at least 4 registered earthquakes for it to be considered a cluster. This will mean we will have more sparse clusters.\n\n\nwarnings.filterwarnings('ignore')\ndf_full = pd.read_csv(\"../../python-notebooks/datasets/earthquakes.csv\")\ndf = df_full[['Latitude','Longitude','Magnitude']]\ndf = df[df.Magnitude &gt;= 3]\n\ndbscan = DBSCAN(eps = 6, min_samples=4).fit(df) # fitting the model\nlabels = dbscan.labels_ # getting the labels\n\n\n  In the graph below we can see our plotted data. Each cluster has its own color, then the singular red points are random earthquakes that don’t belong to a cluster. The larger points are the graph indicate a larger magnitude earthquake. We can use this visualization to see areas that have high earthquake activity.\n\n\n\nCode\nworldmap = gpd.read_file(gpd.datasets.get_path(\"naturalearth_lowres\"))\n# Creating axes and plotting world map\nfig, ax = plt.subplots(figsize=(18, 10))\nworldmap.plot(color=\"gray\", ax=ax)\n\nsns.scatterplot(x=df.Longitude, y=df.Latitude, hue=labels, palette='Set1', size=df.Magnitude, sizes=(10,200)) # plotting the clusters\n\nax. legend_ = None\nplt.title(\"Earthquakes clusters with magnitudes greater than 3 in 2016\")\nplt.xlabel(\"Longitude\") # X-axis label\nplt.ylabel(\"Latitude\") # Y-axis label\n\nplt.show() # showing the plot\nwarnings.filterwarnings('ignore')\n\n\n\n\n\n\n  In this example we used clustering to group equakes together based on their proximity to each other. This is a great example of unsupervised learning. We took a raw data set and clustered the data points based on their similarities. From here we can view the clusters determine some patterns from the data. The algorithm itself doesn’t understand the data but when we view it, especially on the map, we can see the patterns of each cluster. We see that certain area of the world are more prone to earthquakes than others. I hope this blog post was helpful in your understanding of Clustering and unsupervised learning.\n\n \n\nThank you!"
  },
  {
    "objectID": "posts/Classification/index.html",
    "href": "posts/Classification/index.html",
    "title": "Classification",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport numpy as np\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nimport seaborn as sns\nfrom sklearn.cluster import KMeans\nimport warnings\n\n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n \n\nThank you!"
  },
  {
    "objectID": "posts/probability/index.html",
    "href": "posts/probability/index.html",
    "title": "Probablity",
    "section": "",
    "text": "Probability Theory and Random variables are important to machine learning because it is the mathematical foundation for analyzing data. This subject allows data scientists to assess the certainty of outcomes in experiments. One of the ways they can do this is by examining data distributions. Numerical data can come in 2 different forms, discrete or continuous. Discrete data deals with data points that come in a countable form. A good example of a discrete experiment is rolling a dice and seeing what number you get on each roll. Using the Python function diceRoll we can create a Discrete dataset which we can then visualize using a histogram.\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nimport seaborn as sns\ndef diceRoll(n):\n    rollsList = []\n    for i in range(n):\n        two_dice = ( np.random.randint(1, 7) + np.random.randint(1, 7) )\n        rollsList.append(two_dice)\n    return rollsList\n\ndata = diceRoll(10)\n# print(data)\nbins = np.arange(14) - 0.5\nweights = np.ones_like(data) / len(data)\nplt.hist(data, weights=weights, bins=bins, color=\"blue\", rwidth=0.85)\nplt.title(\"Probability of Rolling 2 Dice Histogram\")\nplt.xlabel(\"Sum of 10 2 dice rolls\")\nplt.ylabel(\"Probability\")\nplt.xticks(range(1,14))\nplt.yticks(np.arange(0, .5, step=0.1))\n# Set axes limit\nplt.xlim(1,13)\nplt.show()\n\n\n\n\n\n\nCode\ndata = diceRoll(100)\nbins = np.arange(14) - 0.5\nweights = np.ones_like(data) / len(data)\nplt.hist(data, weights=weights, bins=bins, color=\"blue\", rwidth=0.85)\nplt.title(\"Probability of Rolling 2 Dice Histogram\")\nplt.xlabel(\"Sum of 100 2 dice rolls\")\nplt.ylabel(\"Probability\")\nplt.xticks(range(1,14))\nplt.yticks(np.arange(0, .5, step=0.1))\nplt.xlim(1,13)\nplt.show()\n\ndata = diceRoll(10000)\nbins = np.arange(14) - 0.5\nweights = np.ones_like(data) / len(data)\nplt.hist(data, weights=weights, bins=bins, color=\"blue\", rwidth=0.85)\nplt.title(\"Probability of Rolling 2 Dice Histogram\")\nplt.xlabel(\"Sum of 10000 2 dice rolls\")\nplt.ylabel(\"Probability\")\nplt.xticks(range(1,14))\nplt.yticks(np.arange(0, .5, step=0.1))\nplt.xlim(1,13)\nplt.show()\n\n\n\n\n\n\n\n\n\n  As you can see here with the lower sample size the graph looks more erratic but as the same size increases the distribution becomes more normal. A normal or Gaussian distribution takes on the shape of a bell curve. Also as you can see in the function we are using a random number generator. This randomness is important to any dataset. Random variables help us quantify real-world experiments. They help us capture the center of that random variable distribution with large sample sizes. When we have small sample sizes the random variables don’t help us much, as we can see in the first graph. Once they increase the graph starts to form and we see the true median, which is 7. In this case, our random variables are considered discrete because they only take in values the possible values from a 6-sided dice, 2 - 12.\n\n\n\nCode\ndiamonds = pd.read_csv('../../python-notebooks/datasets/diamonds.csv')\ndiamonds.sort_values(by=[\"price\"], ascending=True, inplace=True)\n\n\n\n  Continuous probability theory deals with data in a continuous data space. A good example of a continuous data set is price. Price is not a fixed number of values, it can be anything that the seller wants to set it as. In a diamond dataset, I have found online, the prices vary drastically. As you can see from the head and the tail of the data set the prices are 2 completely different numbers, but someone could try and charge double the highest price for a diamond.\n\n\ndiamonds.head()\n\n\n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n0\n0.23\nIdeal\nE\nSI2\n61.5\n55.0\n326\n3.95\n3.98\n2.43\n\n\n1\n0.21\nPremium\nE\nSI1\n59.8\n61.0\n326\n3.89\n3.84\n2.31\n\n\n2\n0.23\nGood\nE\nVS1\n56.9\n65.0\n327\n4.05\n4.07\n2.31\n\n\n3\n0.29\nPremium\nI\nVS2\n62.4\n58.0\n334\n4.20\n4.23\n2.63\n\n\n4\n0.31\nGood\nJ\nSI2\n63.3\n58.0\n335\n4.34\n4.35\n2.75\n\n\n\n\n\n\n\n\ndiamonds.tail()\n\n\n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n27745\n2.00\nVery Good\nH\nSI1\n62.8\n57.0\n18803\n7.95\n8.00\n5.01\n\n\n27746\n2.07\nIdeal\nG\nSI2\n62.5\n55.0\n18804\n8.20\n8.13\n5.11\n\n\n27747\n1.51\nIdeal\nG\nIF\n61.7\n55.0\n18806\n7.37\n7.41\n4.56\n\n\n27748\n2.00\nVery Good\nG\nSI1\n63.5\n56.0\n18818\n7.90\n7.97\n5.04\n\n\n27749\n2.29\nPremium\nI\nVS2\n60.8\n60.0\n18823\n8.50\n8.47\n5.16\n\n\n\n\n\n\n\n\n  When we put the data into a distribution plot with price on the x-axis and count on the y-axis we can see a graph that makes a nice exponential distribution. This distribution signals to us that the price can go on forever and be as high as the seller wants.\n\n\nsns.displot(diamonds[\"price\"])\n\n\n\n\n\n  Probability and Random variables are important machine learning concepts. In supervised learning, for example, we like to look at the probability of X happening given observed variables Y. These distributions of observed data help us decipher what the outcome should be. Hopefully this blog has helped you understand more about probability distributions, random variables and their influence on machine learning.\n\n \n\nThank you!"
  },
  {
    "objectID": "posts/Linear Regression/index.html",
    "href": "posts/Linear Regression/index.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Code\nimport pandas as pd\nimport numpy as np\nimport geopandas as gpd\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\nimport seaborn as sns\nfrom sklearn.cluster import KMeans\nimport warnings\n\n\n\n  \n\n\n  \n\n\n  \n\n\n  \n\n \n\nThank you!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ellie’s Machine Learning Blog",
    "section": "",
    "text": "Outlier Detection\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nNov 30, 2023\n\n\nEllie Woodward\n\n\n\n\n\n\n  \n\n\n\n\nClassification\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nNov 23, 2023\n\n\nEllie Woodward\n\n\n\n\n\n\n  \n\n\n\n\nLinear Regression\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nNov 20, 2023\n\n\nEllie Woodward\n\n\n\n\n\n\n  \n\n\n\n\nClustering Earthquake Data\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nNov 10, 2023\n\n\nEllie Woodward\n\n\n\n\n\n\n  \n\n\n\n\nProbablity\n\n\n\n\n\n\n\ncode\n\n\n\n\n\n\n\n\n\n\n\nNov 1, 2023\n\n\nEllie Woodward\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nAug 20, 2023\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  }
]